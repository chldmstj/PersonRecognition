{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Person_Recognition",
      "provenance": [],
      "collapsed_sections": [
        "bHFrEhh-ZZbQ",
        "aJ33S7lGrBcI",
        "0r6qL7y1Z8ky"
      ],
      "mount_file_id": "1BJRagFZBgBErsy7gkOC4CSrsmh8QllM5",
      "authorship_tag": "ABX9TyM6Q/nDeDlM86/EsrAcYXsv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chldmstj/PersonRecognition/blob/main/Person_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHFrEhh-ZZbQ"
      },
      "source": [
        "#Face Detection\n",
        " * face detection model\n",
        " * retina face model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6Dlprl5Zn3O"
      },
      "source": [
        "* model 및 라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkZF7pEWX0H0",
        "outputId": "48526349-6ae1-4779-ff02-58f617e632ea"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/pro1/lecture_mosaic_project_data.zip\" -d \"/content\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/pro1/lecture_mosaic_project_data.zip\n",
            "   creating: /content/lecture_mosaic_project_data/dataset/\n",
            "  inflating: /content/lecture_mosaic_project_data/dataset/bts.jpg  \n",
            "  inflating: /content/lecture_mosaic_project_data/dataset/radio.mp4  \n",
            "  inflating: /content/lecture_mosaic_project_data/dataset/radio_frame.PNG  \n",
            "  inflating: /content/lecture_mosaic_project_data/dataset/radio_frame2.PNG  \n",
            "   creating: /content/lecture_mosaic_project_data/facenet/\n",
            "  inflating: /content/lecture_mosaic_project_data/facenet/facenet_keras.h5  \n",
            "   creating: /content/lecture_mosaic_project_data/retinaface/\n",
            "  inflating: /content/lecture_mosaic_project_data/retinaface/saved_model.pb  \n",
            "   creating: /content/lecture_mosaic_project_data/retinaface/variables/\n",
            "  inflating: /content/lecture_mosaic_project_data/retinaface/variables/variables.data-00000-of-00001  \n",
            "  inflating: /content/lecture_mosaic_project_data/retinaface/variables/variables.index  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP7_wQ47Zmz8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import timeit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlpigeMnZ2qP"
      },
      "source": [
        " * facenet,retinaface 모델 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCCaLs5NZsVu",
        "outputId": "e02f32aa-c4d5-4a12-e2f1-54aec7afb5df"
      },
      "source": [
        "facenet_model = load_model('/content/lecture_mosaic_project_data/facenet/facenet_keras.h5')\n",
        "retinaface_model = load_model('/content/lecture_mosaic_project_data/retinaface')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ33S7lGrBcI"
      },
      "source": [
        "# Detect and Crop faces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52tR_V4brDKw"
      },
      "source": [
        "# 이미지 리스트를 받아 face를 추출\n",
        "def detect_faces_frames(images):\n",
        "  face_detections = []\n",
        "  faces = {}\n",
        "  for i,img in enumerate(imgs):\n",
        "    img = img.astype(np.float32)\n",
        "    img_height = img.shape[0]\n",
        "    img_width = img.shape[1]\n",
        "    face_detection = retinaface_model(np.expand_dims(img, axis=0)).numpy()\n",
        "    face_detections.append(face_detection)\n",
        "    boxes=[]\n",
        "    for face in face_detection:\n",
        "      x1, y1, x2, y2 = int(face[0] * img_width), int(face[1] * img_height), \\\n",
        "                        int(face[2] * img_width), int(face[3] * img_height)\n",
        "      box=(abs(x1),abs(y1),abs(x2),abs(y2))\n",
        "      boxes.append(box)\n",
        "      \n",
        "      cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (0,255,0), 2)\n",
        "      cv2.putText(img,'{}'.format(str(box[0])),(box[0],box[3]),3,1,(0,255,0))\n",
        "    faces[i] = {'box':boxes}\n",
        "    print(faces[i])\n",
        "    cv2_imshow(img)\n",
        "    return faces\n",
        "\n",
        "\n",
        "    # 이미지 프레임 하나를 받아 face를 추출\n",
        "def detect_faces_frame(img):\n",
        "  face_detections = []\n",
        "  faces = {}\n",
        "  img = img.astype(np.float32)\n",
        "  img_height = img.shape[0]\n",
        "  img_width = img.shape[1]\n",
        "  face_detection = retinaface_model(np.expand_dims(img, axis=0)).numpy()\n",
        "  boxes=[]\n",
        "  for face in face_detection:\n",
        "    x1, y1, x2, y2 = int(face[0] * img_width), int(face[1] * img_height), \\\n",
        "                      int(face[2] * img_width), int(face[3] * img_height)\n",
        "    box=(abs(x1),abs(y1),abs(x2),abs(y2))\n",
        "    boxes.append(box)\n",
        "    \n",
        "    cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (0,255,0), 2)\n",
        "    cv2.putText(img,'{}'.format(str(box[0])),(box[0],box[3]),3,1,(0,255,0))\n",
        "    cv2_imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCakhLIRCMGx"
      },
      "source": [
        "# 여러 장의 target faces Embedding ( crop된 이미지 데이터 사용 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdpVd8KoCQWo"
      },
      "source": [
        "# target face img load\n",
        "frames =[]\n",
        "\n",
        "#화사\n",
        "frame0 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/0.png')\n",
        "frame1 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/1.png')\n",
        "frame2 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/2.png')\n",
        "frame3 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/3.png')\n",
        "frame4 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/4.png')\n",
        "frame5 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/5.png')\n",
        "frame6 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/6.jpg')\n",
        "frame7 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/7.png')\n",
        "frame8 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/8.png')\n",
        "frame9 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/9.png')\n",
        "frame10 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/10.png')\n",
        "frame11 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/11.png')\n",
        "\n",
        "\n",
        "#frames = [frame0,frame1,frame2,frame3,frame4,frame5,frame6,frame7,frame8,frame9]\n",
        "frames = [frame2,frame5,frame7,frame8,frame9,frame10]\n",
        "\n",
        "REQUIRED_SIZE = (160, 160)\n",
        "\n",
        "\n",
        "targets = []\n",
        "\n",
        "def get_targets_embedding(frames):\n",
        "  for frame in frames:\n",
        "    # 이미지 전처리\n",
        "    frame = cv2.resize(frame, REQUIRED_SIZE).astype(np.float32)\n",
        "  # 정규화\n",
        "    mean, std = frame.mean(), frame.std()\n",
        "    frame = (frame-mean)/std\n",
        "  # facenet 모델로 예측값 추출\n",
        "    \n",
        "    y_pred = facenet_model(np.expand_dims(frame, axis=0))\n",
        "    target_embedding = y_pred[0].numpy()\n",
        "    targets.append(target_embedding)\n",
        "  \n",
        "  return targets \n",
        "\n",
        "targets = get_targets_embedding(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16EZi2QgcT8u"
      },
      "source": [
        "# Person Detection\n",
        "* keras retina model\n",
        "* coco dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MQUVOjRcfPi"
      },
      "source": [
        "* keras retinanet model load (resnet50)\n",
        "* lib import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p77KaU_0cd8B",
        "outputId": "8ae5ecbb-58fd-482e-eb6a-aa905d79f3d7"
      },
      "source": [
        "!pip install keras_retinanet\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "%matplotlib inline\n",
        "\n",
        "# import keras\n",
        "import keras\n",
        "\n",
        "# import keras_retinanet\n",
        "from keras_retinanet import models\n",
        "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
        "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
        "from keras_retinanet.utils.colors import label_color\n",
        "from keras_retinanet.utils.gpu import setup_gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_retinanet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a1/00b1d6591bef8276f964b16b52a0b63e941c6aec5631aa2b95fc1b17a1aa/keras-retinanet-1.0.0.tar.gz (71kB)\n",
            "\r\u001b[K     |████▋                           | 10kB 20.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 20kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 30kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 40kB 14.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 51kB 11.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 61kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 6.0MB/s \n",
            "\u001b[?25hCollecting keras-resnet==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/76/d4/a35cbd07381139dda4db42c81b88c59254faac026109022727b45b31bcad/keras-resnet-0.2.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras_retinanet) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras_retinanet) (1.4.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from keras_retinanet) (0.29.21)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from keras_retinanet) (7.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from keras_retinanet) (4.1.2.30)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from keras_retinanet) (3.38.0)\n",
            "Requirement already satisfied: keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from keras-resnet==0.2.0->keras_retinanet) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->keras_retinanet) (1.19.4)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->keras_retinanet) (2.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0->keras_retinanet) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0->keras_retinanet) (3.13)\n",
            "Building wheels for collected packages: keras-retinanet, keras-resnet\n",
            "  Building wheel for keras-retinanet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-retinanet: filename=keras_retinanet-1.0.0-cp36-cp36m-linux_x86_64.whl size=162894 sha256=9e81b7356f947eb8129e431ab72673bc29bfdaf76d9a76da81d3dafc0af823f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/1d/fc/496708301dbd84bc2faa258d24d82f39fe46d9701d52287373\n",
            "  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-resnet: filename=keras_resnet-0.2.0-py2.py3-none-any.whl size=20486 sha256=f478d24bac151bde300d277a2e2e13a7640955e1bc4418de8a13d84811e212ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/09/a5/497a30fd9ad9964e98a1254d1e164bcd1b8a5eda36197ecb3c\n",
            "Successfully built keras-retinanet keras-resnet\n",
            "Installing collected packages: keras-resnet, keras-retinanet\n",
            "Successfully installed keras-resnet-0.2.0 keras-retinanet-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbXoQa3ucpMa",
        "outputId": "68d1b0e8-0019-44d5-b3db-f32a06392aa3"
      },
      "source": [
        "# gpu id 값 설정\n",
        "gpu = 0\n",
        "setup_gpu(gpu)\n",
        "\n",
        "# pretrained model load\n",
        "\n",
        "!wget https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5 \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Physical devices cannot be modified after being initialized\n",
            "1 Physical GPUs, 1 Logical GPUs\n",
            "--2020-12-20 09:57:22--  https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/100249425/b7184a80-9350-11e9-9cc2-454f5c616394?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201220%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201220T095722Z&X-Amz-Expires=300&X-Amz-Signature=6bf19269dd70bc78a28dfca8c37533a2dca8135ebaeb5c22426313a8e34ab562&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=100249425&response-content-disposition=attachment%3B%20filename%3Dresnet50_coco_best_v2.1.0.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-12-20 09:57:22--  https://github-production-release-asset-2e65be.s3.amazonaws.com/100249425/b7184a80-9350-11e9-9cc2-454f5c616394?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201220%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201220T095722Z&X-Amz-Expires=300&X-Amz-Signature=6bf19269dd70bc78a28dfca8c37533a2dca8135ebaeb5c22426313a8e34ab562&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=100249425&response-content-disposition=attachment%3B%20filename%3Dresnet50_coco_best_v2.1.0.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.8.139\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.8.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 152662144 (146M) [application/octet-stream]\n",
            "Saving to: ‘resnet50_coco_best_v2.1.0.h5’\n",
            "\n",
            "resnet50_coco_best_ 100%[===================>] 145.59M  84.3MB/s    in 1.7s    \n",
            "\n",
            "2020-12-20 09:57:24 (84.3 MB/s) - ‘resnet50_coco_best_v2.1.0.h5’ saved [152662144/152662144]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_cke8yjcx4O",
        "outputId": "aa8a3690-783a-4564-9231-aeabf3bf2c3b"
      },
      "source": [
        "model_path = '/content/resnet50_coco_best_v2.1.0.h5'\n",
        "\n",
        "# pretrained coco 모델 파일을 retinanet 모델로 로딩.  \n",
        "retina_model = models.load_model(model_path, backbone_name='resnet50')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZhxFdSzpZma"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIpxz6jVc_U1"
      },
      "source": [
        "# Face Detection + Person Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDSScigMdOGB"
      },
      "source": [
        "* data 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46roMMi8dHx6"
      },
      "source": [
        "origin_video='/content/drive/MyDrive/data/videoplayback.mp4'\n",
        "save_videoname = 'person recognition.mp4'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a3Vshw3dW2J"
      },
      "source": [
        "* person 안에 face가 있는지 확인해 주는 함수 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA7E_CzTdcol"
      },
      "source": [
        "def contains(person,face):\n",
        "  return person[0] < face[0] < face[2] < person[2] and person[1] < face[1] < face[3] < person[3]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtf7TAuByJ3l"
      },
      "source": [
        "* frame PADDING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7VwIKxrOsFB"
      },
      "source": [
        "def get_crop_frame(frame, box):\n",
        "  img = frame\n",
        "  result = np.zeros_like(img)\n",
        "  # copy img image into center of result image\n",
        "  result[box[1]:box[3],box[0]:box[2]] = img[box[1]:box[3],box[0]:box[2]]\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrUOqsInujpK"
      },
      "source": [
        "* cosine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh2GVnUVulhy"
      },
      "source": [
        "def cosine(target_vector, detectec_vector):\n",
        "  simul = dot(target_vector,detectec_vector)/(norm(target_vector) * norm(detectec_vector))\n",
        "  return simul"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvQKYMkzAIs2"
      },
      "source": [
        "## face write"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waZCsrCFAIOx"
      },
      "source": [
        "def write_face(i_frame,orig_img,boxes):\n",
        "    target_face = orig_img[boxes[1]:boxes[3],boxes[0]:boxes[2]]\n",
        "\n",
        "    cv2_imshow(target_face)\n",
        "    cv2.imwrite('target_face_{}.jpg'.format(i_frame), target_face)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soLh-tCsujyH"
      },
      "source": [
        ">> Face Detection on Viedo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_zA6pUOu9NW"
      },
      "source": [
        "face_boxes = []\n",
        "cap = cv2.VideoCapture(origin_video)\n",
        "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "#재생할 파일의 높이 얻기\n",
        "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "#재생할 파일의 프레임 레이트 얻기\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "#저장할 비디오 코덱\n",
        "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "\n",
        "#파일 stream 생성\n",
        "out = cv2.VideoWriter(save_videoname, fourcc, fps, (int(width), int(height)))\n",
        "\n",
        "i_frame = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "  ret, frame = cap.read()\n",
        "\n",
        "  if not ret or i_frame >= 300:\n",
        "    break\n",
        "  i_frame+=1\n",
        "  if 0 <= i_frame <= 120 : continue\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  if i_frame % 10 == 0:\n",
        "    print('i_fram', i_frame)\n",
        "\n",
        "  # face detection 용 copy frame  \n",
        "  orig = frame.copy()\n",
        "\n",
        "########################### << Face Detection >> ################################################\n",
        "\n",
        "  input_image = frame.astype(np.float32)\n",
        "  img_height = input_image.shape[0]\n",
        "  img_width = input_image.shape[1]\n",
        "\n",
        "  face_detections = retinaface_model(np.expand_dims(input_image, axis=0)).numpy()\n",
        "\n",
        "  embedding_box_dict = {}\n",
        "\n",
        "# get embedding box list = frame에 있는 얼굴들의 embd 값을 dict에 저장\n",
        "  for i, face in enumerate(face_detections):\n",
        "    x1, y1, x2, y2 = int(face[0] * img_width), int(face[1] * img_height), \\\n",
        "                    int(face[2] * img_width), int(face[3] * img_height)\n",
        "    box = (abs(x1),abs(y1),abs(x2),abs(y2))\n",
        "    \n",
        "    face_tensor = orig[box[1]:box[3], box[0]:box[2]]\n",
        "    face_img = face_tensor.copy()\n",
        "    face_tensor = cv2.resize(face_tensor, REQUIRED_SIZE).astype(np.float32)\n",
        "\n",
        "    mean, std = face_tensor.mean(), face_tensor.std()\n",
        "    face_tensor = (face_tensor - mean) / std\n",
        "\n",
        "    embedding = facenet_model(np.expand_dims(face_tensor, axis=0)).numpy()\n",
        "    embedding_box_dict[i] = {'box' : box, 'embedding': embedding}\n",
        "\n",
        "\n",
        "  # get cosine result list\n",
        "\n",
        "  distance_arraies=[]\n",
        "\n",
        "  for target in targets :\n",
        "    for i in range(0, len(embedding_box_dict)):\n",
        "      distance_arraies.append(cosine(embedding_box_dict[i]['embedding'], target))\n",
        "\n",
        "\n",
        "  # list가 비었다면 (detection한 얼굴이 없을 때)\n",
        "  if not distance_arraies :\n",
        "    # detection 하지 못한 원본 frame을 저장하고 다음 frame으로 넘어감 (frame이 끊기는 것을 방지)\n",
        "    #out.write(face_orig) \n",
        "    continue\n",
        "\n",
        "  most_simularity_index = np.array(distance_arraies).mean(axis=0).argmax()\n",
        "  \n",
        "\n",
        "\n",
        "  if distance_arraies[most_simularity_index] >= 0.6:\n",
        "    target_index = most_simularity_index\n",
        "  else:\n",
        "    # detection 된 face가 모두 임계값을 넘지 않는 다면 = target face가 없다면\n",
        "    # detection 하지 못한 원본 frame을 저장하고 다음 frame으로 넘어감 (frame이 끊기는 것을 방지)\n",
        "    #out.write(face_orig)\n",
        "    continue\n",
        "  \n",
        "  \n",
        "  for index  in embedding_box_dict:\n",
        "    if index == target_index:\n",
        "      # target face의 box 값 추출\n",
        "      face_box = embedding_box_dict[index]['box']\n",
        "      write_face(i_frame,orig,face_box)\n",
        "\n",
        "      \n",
        "cap.release()\n",
        "out.release()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUPK-2J4u0-7"
      },
      "source": [
        ">> Person Detection on Viedo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlerHJD40JSD"
      },
      "source": [
        "cap = cv2.VideoCapture(origin_video)\n",
        "\n",
        "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "#재생할 파일의 높이 얻기\n",
        "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "#재생할 파일의 프레임 레이트 얻기\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "#저장할 비디오 코덱\n",
        "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "\n",
        "#파일 stream 생성\n",
        "out = cv2.VideoWriter(save_videoname, fourcc, fps, (int(width), int(height)))\n",
        "\n",
        "i_frame = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "  ret, frame = cap.read()\n",
        "\n",
        "  if not ret or i_frame >= 300:\n",
        "    break\n",
        "  i_frame+=1\n",
        "  if 0 <= i_frame <= 120 : continue\n",
        "\n",
        "  \n",
        "\n",
        "  if i_frame % 10 == 0:\n",
        "    print('i_fram', i_frame)\n",
        "\n",
        "  # face detection 용 copy frame  \n",
        "  orig = frame.copy()\n",
        "\n",
        "\n",
        "  ################ << Person Detection >> ##################################################\n",
        "\n",
        "  # frame 전처리\n",
        "  img_array = preprocess_image(frame)\n",
        "  img_array, scale = resize_image(img_array)\n",
        "  \n",
        "  # Person Detection을 위한 retina 모델에 frame 전달\n",
        "  boxes, scores, labels = retina_model.predict_on_batch(np.expand_dims(img_array, axis=0))\n",
        "  boxes /= scale\n",
        "\n",
        "  for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
        "    if label == 0: # person일 경우만 detection\n",
        "      if score < 0.5: # 임계값을 넘지 않으면 다음 frame으로\n",
        "        if i_frame % 10 == 0:\n",
        "          print('Fail thres i_frame', i_frame)    \n",
        "          cv2_imshow(orig)\n",
        "        break\n",
        "      color = label_color(label)\n",
        "      person_box = box.astype(int)\n",
        "      if i_frame % 10 ==0:\n",
        "        print(\"Success > i_frame\",i_frame)\n",
        "        \n",
        "        draw_box(orig, person_box, color=color)\n",
        "        cv2_imshow(orig)\n",
        "\n",
        "  # 인식된 이미지 파일로 저장\n",
        "  #out.write(person_orig)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tehcr9sduzs"
      },
      "source": [
        "# Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amI8Ug1idyZU"
      },
      "source": [
        "cap = cv2.VideoCapture(origin_video)\n",
        "\n",
        "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "#재생할 파일의 높이 얻기\n",
        "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "#재생할 파일의 프레임 레이트 얻기\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "#저장할 비디오 코덱\n",
        "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "\n",
        "#파일 stream 생성\n",
        "out = cv2.VideoWriter(save_videoname, fourcc, fps, (int(width), int(height)))\n",
        "\n",
        "i_frame = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "  ret, frame = cap.read()\n",
        "\n",
        "  if not ret or i_frame >= 300:\n",
        "    break\n",
        "\n",
        "  i_frame+=1\n",
        "\n",
        "  if 0 <= i_frame <= 120 : continue\n",
        "  # face detection 용 copy frame  \n",
        "  face_orig = frame.copy()\n",
        "  # person detection 용 copy frame\n",
        "  person_orig = frame.copy()\n",
        "\n",
        "########################### << Face Detection >> ################################################\n",
        "\n",
        "  input_image = frame.astype(np.float32)\n",
        "  img_height = input_image.shape[0]\n",
        "  img_width = input_image.shape[1]\n",
        "\n",
        "  face_detections = retinaface_model(np.expand_dims(input_image, axis=0)).numpy()\n",
        "\n",
        "  embedding_box_dict = {}\n",
        "\n",
        "# get embedding box list = frame에 있는 얼굴들의 embd 값을 dict에 저장\n",
        "  for i, face in enumerate(face_detections):\n",
        "    x1, y1, x2, y2 = int(face[0] * img_width), int(face[1] * img_height), \\\n",
        "                    int(face[2] * img_width), int(face[3] * img_height)\n",
        "    box = (abs(x1),abs(y1),abs(x2),abs(y2))\n",
        "    \n",
        "    face_tensor = face_orig[box[1]:box[3], box[0]:box[2]]\n",
        "    face_img = face_tensor.copy()\n",
        "    face_tensor = cv2.resize(face_tensor, REQUIRED_SIZE).astype(np.float32)\n",
        "\n",
        "    mean, std = face_tensor.mean(), face_tensor.std()\n",
        "    face_tensor = (face_tensor - mean) / std\n",
        "\n",
        "    embedding = facenet_model(np.expand_dims(face_tensor, axis=0)).numpy()\n",
        "    embedding_box_dict[i] = {'box' : box, 'embedding': embedding}\n",
        "\n",
        "\n",
        "  # get cosine result list\n",
        "\n",
        "  distance_arraies=[]\n",
        "\n",
        "  for target in targets :\n",
        "    for i in range(0, len(embedding_box_dict)):\n",
        "      distance_arraies.append(cosine(embedding_box_dict[i]['embedding'], target))\n",
        "\n",
        "\n",
        "  # list가 비었다면 (detection한 얼굴이 없을 때)\n",
        "  if not distance_arraies :\n",
        "    # detection 하지 못한 원본 frame을 저장하고 다음 frame으로 넘어감 (frame이 끊기는 것을 방지)\n",
        "    #out.write(face_orig) \n",
        "    # if i_frame % 10 == 0:\n",
        "    #   print('i_fram', i_frame)\n",
        "    #   cv2_imshow(face_orig)\n",
        "    continue\n",
        "\n",
        "  most_simularity_index = np.array(distance_arraies).mean(axis=0).argmax()\n",
        "  \n",
        "\n",
        "\n",
        "  if distance_arraies[most_simularity_index] >= 0.5:\n",
        "    target_index = most_simularity_index\n",
        "  else:\n",
        "    # detection 된 face가 모두 임계값을 넘지 않는 다면 = target face가 없다면\n",
        "    # detection 하지 못한 원본 frame을 저장하고 다음 frame으로 넘어감 (frame이 끊기는 것을 방지)\n",
        "    #out.write(face_orig)\n",
        "    if i_frame % 10 == 0:\n",
        "      print('fail detect i_fram', i_frame)\n",
        "      cv2_imshow(face_orig)\n",
        "    continue\n",
        "  \n",
        "  for index  in embedding_box_dict:\n",
        "    if index == target_index:\n",
        "      # target face의 box 값 추출\n",
        "      face_box = embedding_box_dict[index]['box']\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  ################ << Person Detection >> ##################################################\n",
        "\n",
        "  # frame 전처리\n",
        "  img_array = preprocess_image(frame)\n",
        "  img_array, scale = resize_image(img_array)\n",
        "  \n",
        "  # Person Detection을 위한 retina 모델에 frame 전달\n",
        "  boxes, scores, labels = retina_model.predict_on_batch(np.expand_dims(img_array, axis=0))\n",
        "  boxes /= scale\n",
        "\n",
        "  for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
        "    if score < 0.5: # 임계값을 넘지 않으면 다음 frame으로\n",
        "      if i_frame % 10 == 0:\n",
        "        print('fail thres i_fram', i_frame)    \n",
        "        cv2_imshow(person_orig)\n",
        "      break\n",
        "    if label == 0: # person일 경우만 detection\n",
        "      color = label_color(label)\n",
        "      person_box = box.astype(int)\n",
        "      if contains(person_box,face_box):\n",
        "        person_orig = get_crop_frame(person_orig,person_box)\n",
        "        if i_frame % 10 ==0:\n",
        "          print(\"success i frame\",i_frame)\n",
        "          cv2_imshow(person_orig)\n",
        "        \n",
        "        #draw_box(person_orig, person_box, color=color)\n",
        "\n",
        "  # 인식된 이미지 파일로 저장\n",
        "  #out.write(person_orig)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}