{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Person_Recognition",
      "provenance": [],
      "collapsed_sections": [
        "bHFrEhh-ZZbQ",
        "aJ33S7lGrBcI",
        "0r6qL7y1Z8ky"
      ],
      "mount_file_id": "1BJRagFZBgBErsy7gkOC4CSrsmh8QllM5",
      "authorship_tag": "ABX9TyMiKM0RcV66F3AKDL44biMq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chldmstj/PersonRecognition/blob/main/Person_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHFrEhh-ZZbQ"
      },
      "source": [
        "#Face Detection\n",
        " * face detection model\n",
        " * retina face model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6Dlprl5Zn3O"
      },
      "source": [
        "* model 및 라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkZF7pEWX0H0",
        "outputId": "48526349-6ae1-4779-ff02-58f617e632ea"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/pro1/lecture_mosaic_project_data.zip\" -d \"/content\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/pro1/lecture_mosaic_project_data.zip\n",
            "   creating: /content/lecture_mosaic_project_data/dataset/\n",
            "  inflating: /content/lecture_mosaic_project_data/dataset/bts.jpg  \n",
            "  inflating: /content/lecture_mosaic_project_data/dataset/radio.mp4  \n",
            "  inflating: /content/lecture_mosaic_project_data/dataset/radio_frame.PNG  \n",
            "  inflating: /content/lecture_mosaic_project_data/dataset/radio_frame2.PNG  \n",
            "   creating: /content/lecture_mosaic_project_data/facenet/\n",
            "  inflating: /content/lecture_mosaic_project_data/facenet/facenet_keras.h5  \n",
            "   creating: /content/lecture_mosaic_project_data/retinaface/\n",
            "  inflating: /content/lecture_mosaic_project_data/retinaface/saved_model.pb  \n",
            "   creating: /content/lecture_mosaic_project_data/retinaface/variables/\n",
            "  inflating: /content/lecture_mosaic_project_data/retinaface/variables/variables.data-00000-of-00001  \n",
            "  inflating: /content/lecture_mosaic_project_data/retinaface/variables/variables.index  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP7_wQ47Zmz8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import timeit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlpigeMnZ2qP"
      },
      "source": [
        " * facenet,retinaface 모델 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCCaLs5NZsVu",
        "outputId": "e02f32aa-c4d5-4a12-e2f1-54aec7afb5df"
      },
      "source": [
        "facenet_model = load_model('/content/lecture_mosaic_project_data/facenet/facenet_keras.h5')\n",
        "retinaface_model = load_model('/content/lecture_mosaic_project_data/retinaface')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ33S7lGrBcI"
      },
      "source": [
        "# Detect and Crop faces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52tR_V4brDKw"
      },
      "source": [
        "# 이미지 리스트를 받아 face를 추출\n",
        "def detect_faces_frames(images):\n",
        "  face_detections = []\n",
        "  faces = {}\n",
        "  for i,img in enumerate(imgs):\n",
        "    img = img.astype(np.float32)\n",
        "    img_height = img.shape[0]\n",
        "    img_width = img.shape[1]\n",
        "    face_detection = retinaface_model(np.expand_dims(img, axis=0)).numpy()\n",
        "    face_detections.append(face_detection)\n",
        "    boxes=[]\n",
        "    for face in face_detection:\n",
        "      x1, y1, x2, y2 = int(face[0] * img_width), int(face[1] * img_height), \\\n",
        "                        int(face[2] * img_width), int(face[3] * img_height)\n",
        "      box=(abs(x1),abs(y1),abs(x2),abs(y2))\n",
        "      boxes.append(box)\n",
        "      \n",
        "      cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (0,255,0), 2)\n",
        "      cv2.putText(img,'{}'.format(str(box[0])),(box[0],box[3]),3,1,(0,255,0))\n",
        "    faces[i] = {'box':boxes}\n",
        "    print(faces[i])\n",
        "    cv2_imshow(img)\n",
        "    return faces\n",
        "\n",
        "\n",
        "    # 이미지 프레임 하나를 받아 face를 추출\n",
        "def detect_faces_frame(img):\n",
        "  face_detections = []\n",
        "  faces = {}\n",
        "  img = img.astype(np.float32)\n",
        "  img_height = img.shape[0]\n",
        "  img_width = img.shape[1]\n",
        "  face_detection = retinaface_model(np.expand_dims(img, axis=0)).numpy()\n",
        "  boxes=[]\n",
        "  for face in face_detection:\n",
        "    x1, y1, x2, y2 = int(face[0] * img_width), int(face[1] * img_height), \\\n",
        "                      int(face[2] * img_width), int(face[3] * img_height)\n",
        "    box=(abs(x1),abs(y1),abs(x2),abs(y2))\n",
        "    boxes.append(box)\n",
        "    \n",
        "    cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (0,255,0), 2)\n",
        "    cv2.putText(img,'{}'.format(str(box[0])),(box[0],box[3]),3,1,(0,255,0))\n",
        "    cv2_imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCakhLIRCMGx"
      },
      "source": [
        "# 여러 장의 target faces Embedding ( crop된 이미지 데이터 사용 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdpVd8KoCQWo"
      },
      "source": [
        "# target face img load\n",
        "frames =[]\n",
        "\n",
        "#화사\n",
        "frame0 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/0.png')\n",
        "frame1 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/1.png')\n",
        "frame2 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/2.png')\n",
        "frame3 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/3.png')\n",
        "frame4 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/4.png')\n",
        "frame5 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/5.png')\n",
        "frame6 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/6.jpg')\n",
        "frame7 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/7.png')\n",
        "frame8 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/8.png')\n",
        "frame9 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/9.png')\n",
        "frame10 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/10.png')\n",
        "frame11 = cv2.imread('/content/drive/MyDrive/data/hwasa (1)/11.png')\n",
        "\n",
        "\n",
        "#frames = [frame0,frame1,frame2,frame3,frame4,frame5,frame6,frame7,frame8,frame9]\n",
        "frames = [frame2,frame5,frame7,frame8,frame9,frame10]\n",
        "\n",
        "REQUIRED_SIZE = (160, 160)\n",
        "\n",
        "\n",
        "targets = []\n",
        "\n",
        "def get_targets_embedding(frames):\n",
        "  for frame in frames:\n",
        "    # 이미지 전처리\n",
        "    frame = cv2.resize(frame, REQUIRED_SIZE).astype(np.float32)\n",
        "  # 정규화\n",
        "    mean, std = frame.mean(), frame.std()\n",
        "    frame = (frame-mean)/std\n",
        "  # facenet 모델로 예측값 추출\n",
        "    \n",
        "    y_pred = facenet_model(np.expand_dims(frame, axis=0))\n",
        "    target_embedding = y_pred[0].numpy()\n",
        "    targets.append(target_embedding)\n",
        "  \n",
        "  return targets \n",
        "\n",
        "targets = get_targets_embedding(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16EZi2QgcT8u"
      },
      "source": [
        "# Person Detection\n",
        "* keras retina model\n",
        "* coco dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MQUVOjRcfPi"
      },
      "source": [
        "* keras retinanet model load (resnet50)\n",
        "* lib import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p77KaU_0cd8B",
        "outputId": "8ae5ecbb-58fd-482e-eb6a-aa905d79f3d7"
      },
      "source": [
        "!pip install keras_retinanet\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "%matplotlib inline\n",
        "\n",
        "# import keras\n",
        "import keras\n",
        "\n",
        "# import keras_retinanet\n",
        "from keras_retinanet import models\n",
        "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
        "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
        "from keras_retinanet.utils.colors import label_color\n",
        "from keras_retinanet.utils.gpu import setup_gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_retinanet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a1/00b1d6591bef8276f964b16b52a0b63e941c6aec5631aa2b95fc1b17a1aa/keras-retinanet-1.0.0.tar.gz (71kB)\n",
            "\r\u001b[K     |████▋                           | 10kB 20.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 20kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 30kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 40kB 14.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 51kB 11.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 61kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 6.0MB/s \n",
            "\u001b[?25hCollecting keras-resnet==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/76/d4/a35cbd07381139dda4db42c81b88c59254faac026109022727b45b31bcad/keras-resnet-0.2.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras_retinanet) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras_retinanet) (1.4.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from keras_retinanet) (0.29.21)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from keras_retinanet) (7.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from keras_retinanet) (4.1.2.30)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from keras_retinanet) (3.38.0)\n",
            "Requirement already satisfied: keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from keras-resnet==0.2.0->keras_retinanet) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->keras_retinanet) (1.19.4)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->keras_retinanet) (2.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0->keras_retinanet) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0->keras_retinanet) (3.13)\n",
            "Building wheels for collected packages: keras-retinanet, keras-resnet\n",
            "  Building wheel for keras-retinanet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-retinanet: filename=keras_retinanet-1.0.0-cp36-cp36m-linux_x86_64.whl size=162894 sha256=9e81b7356f947eb8129e431ab72673bc29bfdaf76d9a76da81d3dafc0af823f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/1d/fc/496708301dbd84bc2faa258d24d82f39fe46d9701d52287373\n",
            "  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-resnet: filename=keras_resnet-0.2.0-py2.py3-none-any.whl size=20486 sha256=f478d24bac151bde300d277a2e2e13a7640955e1bc4418de8a13d84811e212ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/09/a5/497a30fd9ad9964e98a1254d1e164bcd1b8a5eda36197ecb3c\n",
            "Successfully built keras-retinanet keras-resnet\n",
            "Installing collected packages: keras-resnet, keras-retinanet\n",
            "Successfully installed keras-resnet-0.2.0 keras-retinanet-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbXoQa3ucpMa",
        "outputId": "68d1b0e8-0019-44d5-b3db-f32a06392aa3"
      },
      "source": [
        "# gpu id 값 설정\n",
        "gpu = 0\n",
        "setup_gpu(gpu)\n",
        "\n",
        "# pretrained model load\n",
        "\n",
        "!wget https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5 \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Physical devices cannot be modified after being initialized\n",
            "1 Physical GPUs, 1 Logical GPUs\n",
            "--2020-12-20 09:57:22--  https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/100249425/b7184a80-9350-11e9-9cc2-454f5c616394?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201220%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201220T095722Z&X-Amz-Expires=300&X-Amz-Signature=6bf19269dd70bc78a28dfca8c37533a2dca8135ebaeb5c22426313a8e34ab562&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=100249425&response-content-disposition=attachment%3B%20filename%3Dresnet50_coco_best_v2.1.0.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-12-20 09:57:22--  https://github-production-release-asset-2e65be.s3.amazonaws.com/100249425/b7184a80-9350-11e9-9cc2-454f5c616394?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201220%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201220T095722Z&X-Amz-Expires=300&X-Amz-Signature=6bf19269dd70bc78a28dfca8c37533a2dca8135ebaeb5c22426313a8e34ab562&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=100249425&response-content-disposition=attachment%3B%20filename%3Dresnet50_coco_best_v2.1.0.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.8.139\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.8.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 152662144 (146M) [application/octet-stream]\n",
            "Saving to: ‘resnet50_coco_best_v2.1.0.h5’\n",
            "\n",
            "resnet50_coco_best_ 100%[===================>] 145.59M  84.3MB/s    in 1.7s    \n",
            "\n",
            "2020-12-20 09:57:24 (84.3 MB/s) - ‘resnet50_coco_best_v2.1.0.h5’ saved [152662144/152662144]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_cke8yjcx4O",
        "outputId": "aa8a3690-783a-4564-9231-aeabf3bf2c3b"
      },
      "source": [
        "model_path = '/content/resnet50_coco_best_v2.1.0.h5'\n",
        "\n",
        "# pretrained coco 모델 파일을 retinanet 모델로 로딩.  \n",
        "retina_model = models.load_model(model_path, backbone_name='resnet50')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}